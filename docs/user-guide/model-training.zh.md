
# 训练任务

**训练任务**模块是 DeepExtension 提供的可视化无代码界面操作模块，用于微调语言模型。它支持多种微调策略，可无缝对接上传的数据集，在训练全过程中可以清晰地进度监控和模型比较。

---

## 开始新训练

开始一个新的训练任务的步骤如下：

1. 在“模型训练”页面点击 **“开始新训练”**；
2. 从 [训练方法管理](training-methods.zh.md) 中选择一个微调方法；
3. 填写下面的训练参数；
4. 点击 **“运行训练”** 后，任务会以 **后台处理模式** 提交并在后端运行。

### 参数说明表

| **参数名称**             | **含义**                                        | **典型取值 / 范围**                               |
|------------------------|------------------------------------------------|--------------------------------------------------|
| `Base Model` / `MODEL_PATH` | 要微调的基础模型                            | 如 `Qwen1.5-7B`、`LLaMA2`等，需要在基础模型中注册 |
| `Dataset` / `DATASET_PATH` | 用于训练的数据集                              | 已上传的 JSONL 格式数据集                        |
| `LORA_RANK`            | LoRA 适配器矩阵的秩                            | 4、8、16                                          |
| `LOAD_IN_4BIT`         | 是否在训练中使用 4-bit 量化                    | `true` 或 `false`                                |
| `MAX_SEQ_LENGTH`       | 模型允许的最大序列长度                        | 512、1024、2048                                  |
| `MAX_INPUT_LENGTH`     | 输入提示的最大 token 长度                      | 256 – 2048                                       |
| `MAX_CONTENT_LENGTH`   | 训练内容的最大 token 长度                      | 256 – 2048                                       |
| `MAX_SAMPLES`          | 训练集中加载的最大样本数                      | 如 1000、5000，用`-1` 表示加载全部样本                |
| `NUM_GENERATIONS`      | 奖励/验证生成的样本数                          | 1 – 10                                           |
| `MAX_GRAD_NORM`        | 梯度裁剪的最大范数                             | 0.5 – 5.0                                        |
| `EPOCHS`               | 数据集完整迭代的轮数                           | 1 – 50（未设置 `MAX_STEPS` 时生效）              |
| `MAX_STEPS`            | 总训练步数                                     | 100 – 50000                                      |
| `BATCH_SIZE`           | 每批样本数                                     | 1 – 64                                           |
| `GRAD_ACCUM_STEPS`     | 梯度累积步数                                   | 1 – 16                                           |
| `LEARNING_RATE`        | 初始学习率                                     | 如 1e-4、5e-5、2e-5                               |
| `WARMUP_STEPS`         | 学习率预热步数                                 | 如 0 – 5000                                      |
| `WARMUP_RATIO`         | 用于预热的比例                                 | 0.01 – 0.2（未设置 `WARMUP_STEPS` 时生效）       |
| `OUTPUT_DIR`           | 训练输出保存位置（由系统自动管理）             | 系统自动生成                                     |
| `PromptInputColumn`    | 数据集中用作输入提示的字段名                   | 如 `question`、`instruction`、`query`            |
| `PromptOutputColumn`   | 数据集中用作参考答案的字段名                   | 如 `answer`、`response`、`completion`            |

> 以上大多数参数会直接传递给训练逻辑。如需自定义处理，请参考 [实现自定义训练](../developer/implement-own-ai-training-cuda.zh.md)。



---

## 查看训练详情与进度

查看已存在的训练任务：

1. 回到“模型训练”主页
2. 点击对应任务的 **“查看详情”**

有如下三个标签页：

- **训练概览**：显示所有训练参数
- **评估数据**：可视化呈现 loss 曲线、奖励得分和性能指标
- **训练日志**：训练过程生成的原始日志，用于调试与审计

---

## 复制训练任务

快速复制并调整已有训练任务：

1. 在训练详情页面的 **“训练概览”** 标签页底部点击 **“复制训练”**
2. 将自动填充配置，可修改关键参数（如数据集或学习率），运行新的训练任务

这对于 A/B 测试和迭代优化非常实用。

---

## 训练对比

可以对多个训练任务进行比较：

- 在主页选择任意两个或以上的任务
- 点击 **“对比”**，即可同时查看参数、性能指标和输出的差异

---

*DeepExtension — 简化企业级 AI 模型训练的全生命周期流程*
