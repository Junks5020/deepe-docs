
# 定制模型

**定制模型（Customized Models）** 是通过 DeepExtension 的模型训练流程生成的输出结果，通常是与其原始基础模型相关联的 **PEFT 适配器** 或 **检查点（checkpoint）**。这些中间产物表示模型已完成训练，但尚未合并为独立的完整模型。

---

## 总览

每当在 [模型训练](model-training.zh.md) 中完成一次训练任务后，系统会自动保存一个定制模型。

在 **定制模型** 页面中，会展示所有已存储的基于适配器的训练结果。每个条目包含：

- **基础模型名称**：该训练适配器所基于的基础模型
- **训练名称**：在提交训练任务时所填写的名称
- **模型卡片**：从训练配置和评估摘要中提取的元数据
- **系统自动生成的模型名称**，格式如下：

```
[customized_model_name] = [base_model_technical_name]_[train_name]_[YYYYMMDD]_[first4ofTrainingUUID]
```

> 目前，仅支持存储 **基于适配器（adapter-based）** 的训练产物 —— 完整检查点的支持尚未提供，但未来版本可能会加入。

---

## 保存定制模型

将定制模型升级为独立的 **完整模型（Complete Model）**：

1. 点击目标定制模型条目中的 **“保存”**
2. 确认系统自动生成的参数 —— 不需要额外手动输入  
3. 点击 **“确定”** 提交保存任务

该过程以 **后台处理模式** 运行，完成后新模型将出现在 [完整模型](saved-models.zh.md) 页面中。

> **注意：** 保存过程中**不支持量化**，原因如下：
>
> - 目前尚无 **统一标准格式** 用于存储量化后的完整模型；
> - 更灵活的做法是将量化操作放在 **部署阶段**，便于按需调整。

---

## 部署定制模型

部署定制模型使其可用于实时推理：

1. 点击所选模型旁的 **“部署”** 按钮
2. 提供所需的额外参数（如量化配置、部署环境）
3. DeepExtension 会将部署请求转发至配置好的 LLM 部署工）
4. 部署成功后，该模型将出现在 [上线模型](deployed-models.zh.md) 页面中

> **注意：**
>
> - DeepExtension 本身并不直接提供模型服务，所以部署前需要先与外部部署后端进行集成。
> - 使用 **Ollama** 进行部署时，需提供有效的 **模型部署模板文件（Deployment Template File）**，详见 [基础模型](base-models.zh.md)。
> - 必须正确设置 **部署环境**，详见 [部署工具配置](deployment-tool-configuration.zh.md)。
> - 因为目前 Ollama API 支持的量化模型限制，DeepExtension 当前也仅支持以下几种量化模式：`no_quantization`、`q8_0`、`q4_K_M` 和 `q4_K_S` 。
 
---

## 删除定制模型

若需删除某个定制模型：

- 点击该模型条目中的 **“删除”**
- 这将永久删除其适配器或 checkpoint 文件

> 若此模型已被保存为完整模型并出现在 [完整模型](saved-models.zh.md) 页面中，该条目将**不会受到影响**。

---

## 定制模型行为说明

- 定制模型始终 **依赖其基础模型**，在未保存并合并为完整模型前无法独立运行
- 系统生成的模型名称确保了可追溯性和版本管理
- 基于 checkpoint 的完整训练结果保存暂未开放，未来版本可能提供支持

---

*DeepExtension —— 以精确与可追溯性管理您的训练成果*
