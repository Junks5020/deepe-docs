
# ğŸ“ˆ Model Assessment

The **Model Assessment** module in DeepExtension provides a powerful, batch-based framework 
for evaluating and comparing language model outputs using real-world datasets. It is designed 
to help you assess model behavior, quality, and alignment â€” both quantitatively and 
qualitatively â€” at scale.

---

## ğŸ¯ Assessment Modes

DeepExtension supports four flexible assessment modes:

- **Single Candidate**: Generate answers from one model â€” no judgment, just output.
- **Two Candidates**: Generate answers from two models for side-by-side comparison.
- **1 Candidate + 1 Judge**: A judge model evaluates a single model's answer based on 
predefined criteria.
- **2 Candidates + 1 Judge**: A judge model compares both answers and selects the better one 
with explanation or scoring.

---

## ğŸ†• Create a New Assessment

1. Click **"Create a New"** on the Model Assessment page.
2. Select an **assessment mode**, **dataset**, and **sample size** (default: all rows).
3. Choose **Model A**, and if applicable, **Model B** and the **Judge Model** â€” these can be:
   - Third-party API models (e.g., OpenAI, Anthropic, ModelScope, etc.)
   - Local fine-tuned adapters or base models
   - Fully deployed in-house models

4. Define prompts:
   - **Candidate System Prompt**: Standard system prompt for inference (see 
[DeepPrompt](deep-prompt.md)). If blank, a default like *"I am an AI assistant."* is applied.
   - **Candidate User Prompt**: Must include dynamic placeholders like `{{column_name}}` to 
pull values from the dataset (e.g., `Give a short answer to the question: {{question}}`).
   - **Judge System Prompt**: Tells the judge model what criteria to use; can include 
`{{CandidateSystemPrompt}}`.
   - **Judge User Prompt**: Uses placeholders like `{{ResponseA}}`, `{{ResponseB}}`, 
`{{ref_answer}}`, etc. (e.g., `Compare answer A: {{ResponseA}} and B: {{ResponseB}} based on 
reference: {{ref_answer}}`).

---

## ğŸ‘€ Preview Before Execution

Before launching the assessment, you may **preview the result** using up to 5 data samples to 
validate correctness:

- Click **"Preview"**
- Review model outputs and prompt injection
- Adjust parameters as needed

Only when you're satisfied, click **"Submit the Assessment"** â€” the process will then run in 
**batch mode**.

---

## ğŸ“Š View an Assessment

On the Model Assessment main page:

- Click **"View"** to inspect a completed assessment
- Three tabs will be available:
  - **Assessment Overview**: Displays all configuration details
  - **Log**: System logs during the batch process
  - **Result**: Shows final results in table format

---

## ğŸ“ Download Results

Click **"Download"** next to a completed assessment to export results as a `.csv` file.

---

## ğŸ“‹ Copy an Existing Assessment

To replicate and tweak an existing assessment:

- Open any completed assessment and go to the **Assessment Overview** tab
- Click **"Copy"**
- This will pre-fill all values, letting you make quick changes and run another assessment

---

*DeepExtension â€” Scalable, flexible, and explainable model evaluation built for the 
enterprise*

