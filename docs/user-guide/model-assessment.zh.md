
# 模型评估

**模型评估**模块是 DeepExtension 提供的一套强大的批量评估框架，用于通过真实的数据集评估和对比模型的输出表现。该模块旨在帮助用户大规模地从定量与定性两个维度评估模型行为、输出质量与对齐程度。

---

## 评估模式

DeepExtension 支持四种灵活的评估模式：

- **单模型候选项**：从一个模型生成答案，不进行判断，仅输出内容。
- **双模型候选项**：从两个模型生成答案，并进行并排对比。
- **1 候选 + 1 评审**：由一个评审模型根据预设标准评估候选模型的回答。
- **2 候选 + 1 评审**：由评审模型对比两个候选答案，并给出更优选项及评分或解释。

---

## 创建新的评估任务

1. 在“模型评估”页面点击 **“新建评估”**
2. 选择评估模式、评估用的数据集、样本数量（默认使用全部样本）。  
   - 数据集支持包含图片字段，用户可以选择包含图片的数据集以支持视觉模型评估。
3. 选择 **模型 A**，如适用还可选择 **模型 B** 和 **评审模型**。  
   模型选择范围与 [Deep Prompt](deep-prompt.zh.md) 一致，支持（包含视觉模型）：
    - 第三方模型（如 OpenAI、Anthropic、ModelScope 等）
    - 已训练模型
    - 已部署模型
4. 支持选择模型的时候同时选择知识库，保证在需要特定领域知识时，模型回答的内容更准确。
5. 定义提示词（Prompt）：
    - **候选模型系统提示词**：用于推理的系统提示（参见 [DeepPrompt](deep-prompt.zh.md)）。如为空，将自动注入默认系统消息，如 *“我是一个 AI 助手。”*
    - **候选模型用户提示词**：
      - 支持包含动态占位符如 `{{column_name}}`，以从数据集中提取值，例如 `请简要回答问题：{{question}}`，`{{question}}`为数据集中某一列字段名
      - 支持纯自然语言文本提示词，如 `对比图片1和图片2的区别`
      - 如果提示词中涉及到了图片，需要在添加提示词时在下拉框时添加图片字段，例如 `image1`、`image2`，对于图片类型的字段，选择后将自动将图片作为模型输入，并且可以调整图片顺序
    - **评审模型系统提示词**：告知评审模型采用何种评估标准，可包含 `{{CandidateSystemPrompt}}`
    - **评审模型用户提示词**：
      - 使用 `{{ResponseA}}`、`{{ResponseB}}`、`{{ref_answer}}` 等占位符，例如：`根据参考答案 {{ref_answer}}，比较答案 A：{{ResponseA}} 与答案 B：{{ResponseB}}`
      - 如果提示词中涉及到了图片，需要在添加提示词时在下拉框时添加图片字段，例如 `image1`、`image2`，对于图片类型的字段，选择后将自动将图片作为模型输入，并且可以调整图片顺序

---

## 执行前预览

在启动评估前，可使用最多 5 条样本进行预览，验证提示词与模型输出是否正确：

- 点击 **“预览”**
- 查看提示词与模型输出结果
- 如果数据集包含图片字段，预览界面会显示问题对应的图片
- 根据需要进行修改

确认无误后，点击 **“提交评估”**，评估过程将以 **批量模式** 执行。

---

## 查看评估结果

在“模型评估”主页面，点击 **“查看”** 可查看已完成的评估任务，包含三个标签页：

  - **评估概览**：显示所有配置详情
  - **日志**：展示评估过程中的系统日志
  - **结果**：以表格形式展示最终结果，如果数据集包含图片字段，结果页面会显示每个问题对应的图片

---

## 下载评估结果

在评估任务完成后，点击 **“下载”** 可将结果导出为 `.csv` 文件。

---

## 复制现有评估

若想基于现有评估快速调整配置并再次运行：

- 打开已完成的评估任务，进入 **“评估概览”** 标签页
- 点击 **“复制”**
- 所有配置项将自动填充，可快速修改并重新运行评估

---

*DeepExtension — 为企业打造的可扩展、灵活、可解释的大模型评估系统*
